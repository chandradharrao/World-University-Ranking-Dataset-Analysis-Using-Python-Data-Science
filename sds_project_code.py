# -*- coding: utf-8 -*-
"""SDS Project Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SJ536AfttyB9D3qN_WsBlpXFoYz_RlP4

### Libraries and the Dataset

**About the data**

The *Times Higher Education (THE)* university rankings dataset from 2011 to 2016.

A world rank is designated and scores assigned between 0 and 100 for:

* Teaching (teaching/the learning environment)
* International Outlook (international/staff, students and research)
* Research (research/volume, income and reputation)
* Citations (citations/research influence)
* Industry Income (income/knowledge transfer, value not available for all)

The weight of the above factors in calculating the total score:

Teaching: 30%;
International Outlook 7.5%;
Research: 30%;
Citations: 30%;
Industry Income: 2.5%;

      total_score = 0.3*teaching + 0.075*international + 0.3*research + 0.3*citations + 0.025*income
"""

# Commented out IPython magic to ensure Python compatibility.
# Import necessary libraries
import math
import copy
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
!pip install pingouin
import pingouin as pg
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
# %matplotlib inline

# Load the dataset
path = "/content/uni_rankings_dataset.csv"
data = pd.read_csv(path)

# View feature names and the top 10 instances
data.head(10)

"""### Exploratory Data Analysis

#### Feature Analysis
"""

# (instances, features)
print("Dataframe shape: ", data.shape, "\n")
print(data.info())

"""1. `world_rank` : World rank for the university. Contains rank ranges and equal ranks (eg. =94 and 201-250)
2. `university_name` : Name of the university
3. `country` : Country of each university
4. `teaching` : University score for teaching (the learning environment)
5. `international` : University score for international outlook (students, staff, reasearch)
6. `research` : University score for research (volume, income and reputation)
7. `citations` : University score for citations (research influence)
8. `income` : University score for industry income (knowledge transfer)
9. `total_score` : Total score for university, used to determine rank
10. `num_students` : Number of students at the university
11. `student_staff_ratio` : Ratio of students to staff at the university
12. `international_students` : Percentage of students of international origin
13. `female_male_ratio` : Ratio of female students to male students in the university
14. `year` : Year of publishing rankings

#### String Processing
"""

# Data contains '-' values, assign as NaN
data.replace('-', np.NaN, inplace=True)

"""**Process categorical variable values**"""

# Check for inconsistent capitalization
print(*set(data.country.values), sep=' ')

"""No inconsistent capitalization found"""

# Process world_rank
data.world_rank.value_counts().keys()

# Observe year keys and number of instances each
data.year.value_counts()

"""**Process numeric variables values**"""

# Process num_students to remove commas
print("Before processing: ")
print(*data.num_students.values[:20], sep=", ")
print("\n")

# Replace commas with empty string 
data.num_students = [i.replace(',', '') if isinstance(i, str) else i for i in data.num_students.values]
# Convert from str to float, can be changed to int after data cleaning
data.num_students = [float(i) for i in data.num_students.values]

print("After processing 'num_students': ")
print(*data.num_students.values[:20], sep=", ")

# Process international_students
print("Before processing: ")
print(*data.international_students.values[:30], sep=", ")
print("\n")

# Remove percentage symbols from the 'international_students' feature
data.international_students = [i[ : -1] if isinstance(i, str) else i for i in data.international_students.values]
# Convert from str to float
data.international_students = [float(i) for i in data.international_students.values]

print("After processing 'international_students': ")
print(*data.international_students.values[:30], sep=", ")

# Process female_male_ratio 
print("Before processing: ")
print(*data.female_male_ratio.values[ : 20], sep=", ")
print("\n")

# Rename column name to female_ratio
data.rename(columns={'female_male_ratio' : 'female_ratio'}, inplace=True)

# Convert ratio to decimal flaoting values
data.female_ratio = [('0.'+i[ : 2]) if isinstance(i, str) else i for i in data.female_ratio]
data.female_ratio = [float(i) for i in data.female_ratio.values]

print("After processing 'female_ratio': ")
print(*data.female_ratio.values[ : 20], sep=", ")

"""This implies that the dataset consists of rankings from the years 2011-2016

**Convert variable values to appropriate data types**
"""

def check_col_types(df):
  
  """Prints data types of all 
  values of every feature in a
  dataframe""" 
  for col in df.columns:
    types = set()
    for val in df[col].values:
      types.add(type(val))
    print(col, ':', *types)

check_col_types(data)

# Check what values are of type str
print('international: ', [i for i in data.international.values[:20] if isinstance(i, str)])
print('income:', [i for i in data.income.values[:20] if isinstance(i, str)])
print('total_score:', [i for i in data.total_score.values[:20] if isinstance(i, str)])

# Convert all str values to type float or int
data.international = [float(i) for i in data.international.values]
data.income = [float(i) for i in data.income.values]
data.total_score = [float(i) for i in data.total_score.values]

# Check if data types are appropriate after processing
check_col_types(data)

data.head(6)

"""####Handling Missing Values"""

# Separate categorical and numerical variables
categorical_vars = ["world_rank", "university_name", "country", "year"]
numerical_vars = ["teaching", "international", "research", "citations", "income", "total_score", "num_students", "student_staff_ratio", "international_students", "female_ratio"] 

# NaN values in categorical variables
print("Missing categorical values :")
print([data[i].isnull().sum() for i in categorical_vars])

# NaN values in numerical variables
print("Missing numerical values :")
missing_cat = [(i, data[i].isnull().sum()) for i in numerical_vars]
print(*missing_cat, sep='\n')
missing_vars = [var for (var, count) in missing_cat if count!=0]
print("\nVariables with missing values: ", *missing_vars)

"""Check for outliers in each variable to determine the best central tendancy measure as replacement for missing values"""

# Boxplots for numerical variables
sns.set_theme(style="whitegrid")
plt.figure(figsize=(28, 12))

for i, col in enumerate(missing_vars, 1):
  plt.subplot(3, 3, i)
  ax = sns.boxplot(x=data[col])

"""`international` has only 9 missing values and has a relatively symmetric distribution about the mean. Therefore, we decide to fill in missing values with the previous value to maintain the shape of the distribution."""

# Fill NaN in international with previous values
data['international'] = data['international'].fillna(method='ffill').fillna(method='bfill')

"""`num_students`,  `student_staff_ratio` and `international_students` have around 60 missing, and higly right-skewed distributions. We therefore decide to replace NaN with the median of each."""

# Replace missing with the median value
data.num_students.fillna(data.num_students.median(), inplace=True)
data.student_staff_ratio.fillna(data.student_staff_ratio.median(), inplace=True)
data.international_students.fillna(data.international_students.median(), inplace=True)

# Convert float to int now that NaN values are removed
data.num_students = [int(i) for i in data.num_students]

"""Both `female_ratio` and `income` have around 200 missing values. That is nearly 10% of all the instances, and the distribution is still skewed with huge outliers. Since the dataframe is ordered according to rank, we decide to impute the missing score values with the previous or next values in the feature column."""

# Replace missing values with neighbouring values
data.female_ratio = data.female_ratio.fillna(method='ffill').fillna(method='bfill')
data.income = data.income.fillna(method='ffill').fillna(method='bfill')

"""Over 54% of total_score is missing. Intuitively, `total_score` and `world_rank` establish the same correlation. Therefore, we decide to calculate the `total_score` values using the weightage method of designation of the overall score using the other scores in the dataset by the *THE*."""

# Replace total_score with transformation on the other score features, no resulting NaN values
data.total_score = 0.3*data.teaching + 0.075*data.international + 0.3*data.research + 0.3*data.citations + 0.025*data.income

data.head()

check_col_types(data)
print("\n")
data.isnull().sum()

"""### Data Visualization

**Observing the difference in female populations in the UK and Germany throughout the years**
"""

# Plot a histogram with both countries' university female_ratios
data_female = data.loc[(data['country'] == 'Germany' ) | (data['country'] == 'United Kingdom')]
ax = sns.displot(data_female,
          x = "female_ratio",
          hue = 'country')

"""We can infer from the graph that Germnay has a supposedly slightly higher female population ratio in its universities. The data that is available for the UK is greater than that for the Germany. Both averages are higher than 0.5.

**Observing the frequency of universities featured in the top 10, grouped by country**
"""

# Create a dataframe of the top 10 universities
top_10 = [str(i) for i in range(11)]
featured_df = data.loc[data.world_rank.isin(top_10)].loc[:, ["university_name", "world_rank", "country"]]

# Get the frequency and countries of universities in the dataframe
frequency = featured_df.university_name.value_counts()
countries = list()
for i in frequency.index.to_list():
  countries.append(data.loc[data.university_name == i].country.iloc[0])

# Plot a bargraph of frequency against university by country
plt.figure(figsize=(6, 6))
plt.title("Frequency of the Top 10 Universities by Country")
sns.set_theme(style="darkgrid")
fig = sns.barplot(x = frequency.values,
                y = frequency.index.to_list(),
                hue = countries)

"""We can infer that 8 universities were featured in the top 10 all the years the rankings were listed. Most of the universities are from the US, 3 are from the UK and ETH Zurich is from Switzerland.

**Analyzing scores assigned to the top 10 universities in the year 2016**
"""

# Access the top 10 universities from the year 2016
data_top10_2016 = data.loc[data['year'] == 2016].head(10)
data_top10_2016_univs = []
for i in data_top10_2016['university_name']:
  data_top10_2016_univs.append(i)

# Create a tuple of titles for plots
myTitles = tuple(data_top10_2016_univs)

# Create subplots
fig = make_subplots(rows = 2,
                    cols = 5,
                    subplot_titles = myTitles,
                    specs = [[{'type':'polar'}, {'type':'polar'}, {'type':'polar'}, {'type':'polar'}, {'type':'polar'}],
                            [{'type':'polar'}, {'type':'polar'}, {'type':'polar'}, {'type':'polar'}, {'type':'polar'}]])
colCnt = 0
rowCnt = 1
modified_numeric_col = copy.deepcopy(numerical_vars)

# Remove num_students because of different scale
modified_numeric_col.remove("international_students")
modified_numeric_col.remove("num_students")
modified_numeric_col.remove("female_ratio")
modified_numeric_col.remove("student_staff_ratio")

# For the top 10 universities
for j in range(len(data_top10_2016_univs)): 
  # For each university
  data_particular_univ = data_top10_2016.iloc[j] 
  # Array for each year from which feature with lowest and highest score can be found out
  Rvals = []
  for k in modified_numeric_col:
    Rvals.append(data_particular_univ[k])
  minR = min(Rvals)
  maxR = max(Rvals)
  spy_df = pd.DataFrame(dict(x = [data_particular_univ[x] for x in modified_numeric_col],
                             theta = modified_numeric_col))
  colCnt = colCnt+1
  if(colCnt > 5):
    rowCnt = 2
    colCnt = 1
  fig.add_trace(go.Scatterpolar(r = [data_particular_univ[x] for x in modified_numeric_col],
                            theta = modified_numeric_col,
                            fill = 'toself'),
                row = rowCnt,
                col = colCnt)

# Update and show plots
fig.update_layout(height=1000, 
                width=4200, 
                title_text="Scores assigned to the top 10 universitites in the year 2016\n")
fig.show()

"""We can infer from the obtained plots that many colleges have relatively lower scores of income. Income scores are very high for CalTech and MIT, as expected. All universities have very higher citation and research scores, meaning they all contribute extremely well to the scientific world. European universities are seen to have higher international scores.

### Normalization and Standardization

**Mean and variance of each feature column**
"""

def ColumnsMeanFinder(data):
  
  '''Print the mean of every column'''
  col_means = data.mean(axis = 0)
  print(col_means)

ColumnsMeanFinder(data)

def ColumnVarFinder(data):
  
  '''Print the variance of each column'''
  col_vars = data.var()
  print(col_vars)

ColumnVarFinder(data)

"""**Normalizing (max-min scaling) all numerical feature values**

Observe the unnormalized data and then the normalized data using bar charts for better understanding
"""

numeric_cols = numerical_vars

# Print out the max and min values of each numeric feature
print("Before Normalization:")
for i in numeric_cols:
  print("For " + i + ":\nMax value: " + str(data[i].max()) + " Min value: " + str(data[i].min()))

def normalize(data, numeric_cols):

  '''Normalize each numeric feature column 
    passed into the function'''
  for i in numeric_cols:
    col_mean = data[i].mean()
    minVal = data[i].min()
    maxVal = data[i].max()
    range = maxVal - minVal
    data[i] = (data[i] - minVal) / range

# Normalize all numeric columns in the dataframe
normalize(data, numeric_cols)

# Observe max and min values after normalization
print("\nAfter Normalization")
for i in numeric_cols:
  print("For " + i + ":\nMax value: " + str(data[i].max()) + " Min value: " + str(data[i].min()))

# Observe each column mean and variance again
print("Means:")
ColumnsMeanFinder(data)
print("\nVariance:")
ColumnVarFinder(data)

"""> **Why is normalization necessary?**<br>It is done in order to bring all the values of a numeric column in the data set to a common scale between 0 and 1. Normalization does so without altering differences in the ranges of values in each column. Transforming every numeric column to a common scale facilitates better comparision, visualization and faster learning the relationships among feature columns.

**Supplementary Functions**
"""

def log_transform(data, columns):

  '''Transform skewed data with the 
  log transformation for a more
  normal distribution'''
  for col in columns:
    data[col] = np.where(data[col] > 0, data[col], 0.0000000001)
    data[col] = np.where(data[col] > 0.0000000001, np.log(data[col]), 0)

#Square root positively skewed data like research
def squareRootCol(data,colNames):
  for i in colNames:
    data[i] = np.sqrt(data[i])

def squareCol(data,colNames):
  for i in colNames:
    data[i] = np.square(data[i]) 

# Standardizing the data
import statistics
def standardize(data, numeric_cols):

  '''Normalize each numeric feature column 
    passed into the function'''
  for i in numeric_cols:
    col_mean = data[i].mean()
    col_std = np.std(data[i])
    data[i] = (data[i] - col_mean) / col_std

skewed_cols = ['num_students', 'international_students', 'student_staff_ratio','citations']

"""**Visualizing normalized features**"""

# Histograms for numerical variables
fig, axes = plt.subplots(2, 5, figsize=(27, 12))
axes = axes.ravel() 

for id in range(len(numerical_vars)):
  col = numerical_vars[id]
  sns.histplot(data, x=col, color="dodgerblue", kde=True, ax=axes[id])
  axes[id].set_title("Normalized "+col)

"""### Statistical Hypothesis Testing

**Research Problem Statement** : *Is the average percentage of international students in UK universities higher than that in the US?*

> ***Null hypothesis***<br>There is no difference between the average percentage of US and UK university students who are of international origin over the years.<br><br>***Alternative hypothesis***<br>The average percentage of UK university students who are of international origin over the years, is higher than in the US.

**Mathematical Representation:**


> $x :$ Set of UK international_students<br>$y :$ Set of US international_students<br><br>$H_o : \mu_x = \mu_y$<br>$H_1 : \mu_x > \mu_y$

**Graphical Depiction**
"""

dataUS = data.loc[data['country'] == 'United States of America']
dataUK = data.loc[data['country'] == 'United Kingdom']

interUS = dataUS['international_students']
interUK = dataUK['international_students']

fig, axes = plt.subplots(1, 3, figsize=(21, 5))
axes = axes.ravel() 

sns.histplot(data, x=interUS, color="dodgerblue", kde=True, ax=axes[0])
ax0 = axes[0].set_title("International Population in US Universities")
sns.histplot(data, x=interUK, color="dodgerblue", kde=True, ax=axes[1])
ax1 = axes[1].set_title("International Population in UK Universities")
sns.boxplot(data=data.loc[data.country.isin(['United States of America', 'United Kingdom'])],
           x='country',
           y='international_students',
           ax=axes[2])
ax1 = axes[2].set_title("Distribution of International Populations")

"""**Measuring statistical significance of the observed results using the Welch t-test**

The samples here are the measured observations of international_students of the countries US and UK. They are drawn independently and the means, variances and sample sizes are calculated below.
"""

# Set the statistical significance (alpha) for the test
alpha = 0.005

# Calculate the length of each feature vector
sizeUS = len(interUS)
sizeUK = len(interUK)

# Observe that sample sizes are different
print('Sample sizes: US = {}, UK = {}'.format(sizeUS, sizeUK))

# Calculate means of both samples
meanUS = interUS.mean()
meanUK = interUK.mean()

# Calculate variance of both samples
varUS = interUS.var()
varUK = interUK.var()

# Observe that the variances are unequal
print('Sample means: US = {}, UK = {}'.format(meanUS, meanUK))
print('Sample variance: US = {}, UK = {}'.format(varUS, varUK))

"""The sample sizes are different. These are unequal samples and therefore cannot be subjected to the Student's t-test. The Student's t-test assumes sample sizes and variances to be equal and here, this is not the case.

Since both sample sizes are large (> 30), the means are both approximately
normally distributed. The samples are independent, and therefore the null
distribution of the difference of the means follows a normal distribution.

The condition for normal distribution is satisfied and we decide to use the Welsh t-test, which performs better when sample sizes and variances are unequal.

The t-score is calculated using, <br><br>
$t_{score} = \frac {\bar {X}_1 - \bar {X}_2} {s_{Welch}}$<br><br>where $\bar {X}_1 - \bar {X}_2$ is the difference of sample means, and, <br><br>
$s_{Welch}  = \sqrt{\frac{s^2_1} {n_1}+\frac{s^2_2} {n_2}}$
"""

# Find the p value from Welch t-test
t_score = stats.ttest_ind_from_stats(interUS.mean(), interUS.std(), interUS.shape[0],
                                   interUK.mean(), interUK.std(), interUK.shape[0],
                                   equal_var=False)

# one-sided p value is half of the returned two-sided p value
p_value = t_score.pvalue / 2
print(t_score.statistic, p_value)

# Either reject or fail to reject the null hypothesis using p value 
if p_value < alpha:
  print('Null hypothesis has enough evidence to be rejected.\n Null hypothesis rejected.')
else:
  print('Null hypothesis does not have enough evidence to be rejected.\n Failed to reject null hypothesis.')

"""$ p < 0.005 $

Since our p value was far lesser than the statistical significance value, we can reliably reject the null hypothesis, in favour of the alternative.

Additionally, we obtained a negative t-test statistic score, which implies that the mean of international students in UK universities is greater than in US universities.

We reject $H_o : \mu_x = \mu_y$  , where<br><br>$x :$Set of UK international_students<br>$y :$ Set of US international_students

### Correlation
"""

# Table for the pair-wise correlation of features x and y
corr_data = pg.pairwise_corr(data,columns=numerical_vars)
corr_data

"""**Analyze the correlation between `teaching` and `total_score`**"""

# Plot a scatter plot between teaching and total_score
top2011 = data.loc[data['year'] == 2011].head(100)

x = top2011['total_score']
y = top2011['teaching']

plt.figure(figsize=(5, 5))
ax = sns.scatterplot(x=x, 
                   y=y)

"""Hence, we can conclude that universities with greater overall score have better teaching scores. The relation follows a roughly linear path with positive slope.


*This suggests a positive correlation between the variables.*

We can infer that universities that are ranked higher up have better learning and educational environments.

**Analyze the correlation between `research` and `citations` in 2015 and 2016**
"""

# Select data from 2015 and 2016
data2015 = data.loc[data['year'] == 2015]
data2016 = data.loc[data['year'] == 2016]

# Pick out citations and research score data
x_2015 = data2015['citations']
y_2015 = data2015['research']
x_2016 = data2016['citations']
y_2016 = data2016['research']

# Plot scatterplots for both years
plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.title("Relation between research and citations in 2015")
ax = sns.scatterplot(x = x_2015, 
                   y = y_2015)
plt.subplot(1, 2, 2)
plt.title("Relation between research and citations in 2016")
ax = sns.scatterplot(x = x_2016, 
                   y = y_2016)
plt.show()

"""We can infer from the plotted graphs that the research score approximately follows an exponential curve, indicating that a higher `citation` score (how much a university is contributing to the sum of human knowledge), the higher the `research` score (a university’s reputation for research excellence among its peers). 

*This suggests a positive correlation between the variables.*

This does have exceptions however, where universities with a high `citation` score in the lower right part of the plots had a very low `research` score. A couple universities with very low `citation` scores had relatively high `research`scores.

**Analyze the correlation between `student_staff_ratio` and `teaching`**
"""

# Plot a scatter plot between student_staff_ratio and international_students
top2015 = data.loc[data['year'] == 2015].head(100)
top2016 = data.loc[data['year'] == 2016].head(100)

x_2015 = top2015['teaching']
y_2015 = top2015['student_staff_ratio']

x_2016 = top2016['teaching']
y_2016 = top2016['student_staff_ratio']

plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.title("Relation between student_staff_ratio and teaching in 2015")
ax = sns.scatterplot(x = x_2015, 
                   y = y_2015)
plt.subplot(1, 2, 2)
plt.title("Relation between student_staff_ratio and teaching in 2016")
ax = sns.scatterplot(x = x_2016, 
                   y = y_2016)
plt.show()

"""We can infer from this graph that teaching scores and student_staff_ratio are negatively correlated. Universities with higher teaching scores and relatively better learning environments have relatively lesser staff employed.

### Resources Used

The Pitfalls of Data Normalization : 
https://radiant-brushlands-42789.herokuapp.com/towardsdatascience.com/pitfalls-of-data-normalization-bf05d65f1f4c

How low can a p-value go? : https://stats.stackexchange.com/questions/11812/sanity-check-how-low-can-a-p-value-go

Welch t - testing Notes : https://ocw.mit.edu/resources/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/lectures-and-labs/MITRES_6_009IAP12_lab3a.pdf
"""